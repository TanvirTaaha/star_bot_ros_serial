{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "YOLOV5_PATH = Path(\"/Users/taaha/codes/pytorch/ultralytics_yolov5/\")\n",
    "import sys\n",
    "sys.path.append(str(YOLOV5_PATH))\n",
    "import detect as dt\n",
    "from utils.dataloaders import LoadWebcam\n",
    "from utils.plots import Colors\n",
    "weights = YOLOV5_PATH / 'runs' / 'train' / 'amin_ds' / 'weights' / 'best.pt'\n",
    "data = YOLOV5_PATH / 'datasets' / 'amin_ds' / 'amin_ds.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/Users/taaha/codes/ros/project_ws/src/ros_serial_test/scripts/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.1-395-g4d05472 Python-3.8.13 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Model summary: 444 layers, 86200330 parameters, 0 gradients, 203.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1: 960x1280 1 paper, 2 plastic_bottles\n",
      "Speed: 20.6ms pre-process, 1486.9ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
      "       xcenter     ycenter       width      height  confidence  class  \\\n",
      "0  1002.861267  688.384277  149.987915  260.274902    0.908981      2   \n",
      "1   510.019592  500.138184  128.642242  145.350952    0.292232      1   \n",
      "2    82.097855  714.043335  164.195709  485.542480    0.287481      2   \n",
      "\n",
      "             name  \n",
      "0  plastic_bottle  \n",
      "1           paper  \n",
      "2  plastic_bottle  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Model\n",
    "model = torch.hub.load(str(YOLOV5_PATH), 'custom', path=str(weights), source='local') # local repo\n",
    "# Images\n",
    "img = cv2.imread('./table.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY )\n",
    "# Inference\n",
    "results = model(img, size=640)  # includes NMS\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "# results.show()  # or .show()\n",
    "\n",
    "#results = results.xyxy[0]  # img1 predictions (tensor)\n",
    "boxes = results.pandas().xywh[0]  # img1 predictions (pandas)\n",
    "print(boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.pandas().xywh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print('hhhhhhh')\n",
    "print(results.pandas().xywh[0].itertuples())\n",
    "\n",
    "\n",
    "# respd.reset_index()\n",
    "for idx, (xc, yc, w, h, conf, cls, name) in results.pandas().xywh[0].iterrows():\n",
    "    print(f\"xcenter:{xc}, ycenter:{yc}, width:{w}, height:{h}, confidence:{conf*100:.2f}%, class:{cls}, name:{name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(905, 95) (996, 62)\n",
      "(960, 1280, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = cv2.imread('./table.jpg')\n",
    "xc, yc = 900, 100\n",
    "\n",
    "im = cv2.circle(img=im, center=(xc,yc), radius=10,color=(0,0,255), thickness=-1)\n",
    "\n",
    "im_h, im_w, channel = im.shape\n",
    "name = 'Hello'\n",
    "text_size = cv2.getTextSize(name, cv2.FONT_HERSHEY_SIMPLEX, fontScale=(1.1e-3 * im.shape[0]), thickness=2)\n",
    "tx_width, tx_height = text_size[0][0], text_size[0][1]\n",
    "pt1 = (((xc+5) if (xc+10+tx_width < im_w) else (xc-tx_width-5)), ((yc-5) if (yc-10-tx_height > 0) else (yc+5+tx_height))) # bottom-left\n",
    "pt2 = (pt1[0]+10+tx_width, pt1[1]-10-tx_height) # top-right\n",
    "pt3 = (pt1[0], pt1[1]-10-tx_height) # top-left\n",
    "pt4 = (pt1[0]+10+tx_width, pt1[1]) # bottom-right\n",
    "# region = np.array([list(pt1),list(pt3),list(pt2),list(pt4)], dtype='int32')\n",
    "region = np.array([pt1,pt3,pt2,pt4], dtype='int32')\n",
    "print(pt1,pt2)\n",
    "print(im.shape)\n",
    "\n",
    "im = cv2.rectangle(img=im, pt1=pt1, pt2=pt2, color=(0,0,255))\n",
    "im = cv2.fillPoly(img=im, pts=[region], color=(0,0,255))\n",
    "im = cv2.putText(img=im, text=name, org=(pt1[0]+5, pt1[1]-5), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=(1.1e-3 * im.shape[0]), color=(255,255,255), thickness=2)\n",
    "\n",
    "cv2.imwrite('image.png', im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = Colors()\n",
    "def add_dot_and_label(img, xc, yc, conf, cls, name, clr=None):\n",
    "    if not clr: clr = colors(int(cls))\n",
    "    label = f\"{name}:{conf*100:.2f}%\"\n",
    "\n",
    "    im_h, im_w, channel = img.shape\n",
    "    text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, fontScale=(1.1e-3 * im_h), thickness=2)\n",
    "    tx_width, tx_height = text_size[0][0], text_size[0][1]\n",
    "\n",
    "    pt1 = (((xc+5) if (xc+10+tx_width < im_w) else (xc-tx_width-5)), ((yc-5) if (yc-10-tx_height > 0) else (yc+5+tx_height))) # bottom-left\n",
    "    pt2 = (pt1[0]+10+tx_width, pt1[1]-10-tx_height) # top-right\n",
    "    pt3 = (pt1[0], pt1[1]-10-tx_height) # top-left\n",
    "    pt4 = (pt1[0]+10+tx_width, pt1[1]) # bottom-right\n",
    "    # region = np.array([list(pt1),list(pt3),list(pt2),list(pt4)], dtype='int32')\n",
    "    region = np.array([pt1,pt3,pt2,pt4], dtype='int32')    \n",
    "\n",
    "    img = cv2.circle(img=img, center=(xc,yc), radius=10,color=clr, thickness=-1)\n",
    "    img = cv2.rectangle(img=img, pt1=pt1, pt2=pt2, color=clr)\n",
    "    img = cv2.fillPoly(img=img, pts=[region], color=clr)\n",
    "    img = cv2.putText(img=img, text=label, org=(pt1[0]+5, pt1[1]-5), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=(1.1e-3 * im_h), color=(255,255,255), thickness=2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n",
      "type of image<class 'numpy.ndarray'>, dim:(1080, 1920, 3)\n",
      "type of tensor<class 'torch.Tensor'>, dim:torch.Size([1080, 1920, 3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/taaha/codes/ros/project_ws/src/ros_serial_test/scripts/python_test.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taaha/codes/ros/project_ws/src/ros_serial_test/scripts/python_test.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ret, img \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taaha/codes/ros/project_ws/src/ros_serial_test/scripts/python_test.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/taaha/codes/ros/project_ws/src/ros_serial_test/scripts/python_test.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pred \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taaha/codes/ros/project_ws/src/ros_serial_test/scripts/python_test.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtype of image\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m, dim:\u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taaha/codes/ros/project_ws/src/ros_serial_test/scripts/python_test.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m mps_img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(img)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/codes/pytorch/ultralytics_yolov5/models/common.py:626\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, imgs, size, augment, profile)\u001b[0m\n\u001b[1;32m    622\u001b[0m t\u001b[39m.\u001b[39mappend(time_sync())\n\u001b[1;32m    624\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(autocast):\n\u001b[1;32m    625\u001b[0m     \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, augment, profile)  \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     t\u001b[39m.\u001b[39mappend(time_sync())\n\u001b[1;32m    629\u001b[0m     \u001b[39m# Post-process\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/pytorch/ultralytics_yolov5/models/common.py:460\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[0;34m(self, im, augment, visualize, val)\u001b[0m\n\u001b[1;32m    457\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mhalf()  \u001b[39m# to FP16\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/pytorch/ultralytics_yolov5/models/yolo.py:136\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m    135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/codes/pytorch/ultralytics_yolov5/models/yolo.py:159\u001b[0m, in \u001b[0;36mModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 159\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m    160\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/pytorch/ultralytics_yolov5/models/common.py:158\u001b[0m, in \u001b[0;36mC3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv3(torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(x)), \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/pytorch/ultralytics_yolov5/models/common.py:111\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x))\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/pytorch/ultralytics_yolov5/models/common.py:50\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/codes/ros/envs/ros/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "batch_size = 1\n",
    "images = []\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pred = model(img)\n",
    "    print(f\"type of image{type(img)}, dim:{img.shape}\")\n",
    "\n",
    "    mps_img = torch.from_numpy(img).to(torch.device('mps'))\n",
    "    print(f\"type of tensor{type(mps_img)}, dim:{mps_img.shape}\")\n",
    "    for idx, (xc, yc, w, h, conf, cls, name) in pred.pandas().xywh[0].iterrows():\n",
    "        img = add_dot_and_label(img, int(xc), int(yc), conf, cls, name)\n",
    "        cv2.imshow('video with bboxes', img)\n",
    "    \n",
    "    if cv2.waitKey(1) == 27: \n",
    "        break  # esc to quit\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "\n",
    "model.to(device)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = dt.check_img_size((640, 640), s=stride)  # check image size\n",
    "\n",
    "dataset = LoadWebcam(pipe=0)\n",
    "bs = len(dataset)\n",
    "\n",
    "model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n",
    "seen, windows, dt = 0, [], [0.0, 0.0, 0.0]\n",
    "for path, im, im0s, vid_cap, s in dataset:\n",
    "    t1 = dt.time_sync()\n",
    "    im = torch.from_numpy(im).to(device)\n",
    "    im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    t2 = dt.time_sync()\n",
    "    dt[0] += t2 - t1\n",
    "    pred = model(im)\n",
    "    t3 = dt.time_sync()\n",
    "    dt[1] += t3 - t2\n",
    "    pred = dt.non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "    dt[2] += dt.time_sync() - t3\n",
    "\n",
    "    for i, det in enumerate(pred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def inv_kinematics(r, z, l1=20, l2=22):\n",
    "        if not (-1 <= (r**2+z**2-l1**2-l2**2)/(2*l1*l2) <= 1):\n",
    "            # domain test of acos\n",
    "            # also limit that is possible for the two links to reach\n",
    "            return\n",
    "\n",
    "        # Equations for Inverse kinematics\n",
    "        phi_2 = (math.acos((r**2+z**2-l1**2-l2**2)/(2*l1*l2)))  # eqn 2\n",
    "        phi_1 = math.atan2(z, r) + math.atan2(l2*math.sin(phi_2), (l1 + l2*math.cos(phi_2))) # eqn 3\n",
    "        phi_2 = phi_2\n",
    "        #theta_1 = rad2deg(phi_2-phi_1)  # eqn 4 converted to degrees\n",
    "\n",
    "        #phi_3 = arccos((r1**2-arm1**2-arm1**2)/(-2*arm1*arm2))\n",
    "        #theta_2 = 180-rad2deg(phi_3)\n",
    "\n",
    "        theta1 = math.degrees(phi_1)\n",
    "        theta2 = math.degrees(phi_2)\n",
    "\n",
    "        #print('theta 1: ', phi_1)\n",
    "        #print('theta two: ', phi_2)\n",
    "\n",
    "        return theta1, theta2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119.0893533346236, 128.18348243320992)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_kinematics(r=12, z=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ros': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54113d50e426ac89d11d63cd20cae8898b8d32eb39c6e954cbcf72a537d59680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
